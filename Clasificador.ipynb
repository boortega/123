{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Jorge Antonio Barajas Avila: 422131531\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificador (SOM)\n",
    "\n",
    "Clasificador</a> by <span property=\"cc:attributionName\">Miguel Angel Pérez León</span> is licensed under <a href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/?ref=chooser-v1\" target=\"_blank\" rel=\"license noopener noreferrer\" style=\"display:inline-block;\">CC BY-NC-SA 4.0<img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/nc.svg?ref=chooser-v1\"><img style=\"height:22px!important;margin-left:3px;vertical-align:text-bottom;\" src=\"https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1\"></a></p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "103b6e1a7514047e480cb26b637f202f",
     "grade": true,
     "grade_id": "cell-51e0133df82c9254",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bibliotecas que se van a usar\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from macti.evaluation import *\n",
    "\n",
    "path = \"./Textos\"\n",
    "# En caso de no existir el directorio de docs\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "\n",
    "    # Se crea el directorio\n",
    "    os.makedirs(path)\n",
    "    \n",
    "    # Se descargan los archivos\n",
    "    for i in range(1,12):\n",
    "        nombre = 'texto'+str(i)+'.txt'\n",
    "        url = 'https://raw.githubusercontent.com/jugernaut/Induccion_MeIA/angel/utils/data/textosMike/'+nombre\n",
    "        response = requests.get(url)\n",
    "        open(path+'/'+nombre, \"wb\").write(response.content)\n",
    "\n",
    "quizz = Quizz('1', 'ManejoDeDatosPractica')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objetivos\n",
    "\n",
    "El propósito de este ejercicio es que el algoritmo que escribas, sea capaz de leer los documentos de la carpeta *Textos* que se descargan al ejecutar la celda superior, y mediante un *SOM* pueda clasificar cada uno de los documentos de manera automática."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 Puntos) Clase `Documento`\n",
    "\n",
    "Genera una clase de *python* llamada `Documento` que contenga la información de un documento de texto. Esta clase debe tener los siguientes atributos:\n",
    "\n",
    "* `ruta`: Ruta del documento a procesar.\n",
    "* `texto`: Texto del documento, en minúsculas.\n",
    "* `vector_caracteristico`: Lista de frecuencias de las palabras del documento, de aceurdo con el `diccionario_universal`.\n",
    "\n",
    "Además, debe tener los siguientes métodos:\n",
    "\n",
    "* `__init__(self, ruta)`: Constructor de la clase. Recibe como parámetro la ruta del documento a procesar. Debe inicializar los atributos `ruta`, `texto` y `vector_caracteristico`.\n",
    "* `__str__(self)`: Método que devuelve la ruta del documento y su vector característico.\n",
    "* `preprocesar(self)`: Método que realiza el preprocesamiento del documento. Debe realizar las siguientes tareas:\n",
    "    * Leer el archivo de texto.\n",
    "    * Convertir el texto a minúsculas.\n",
    "* `data_mining(self)`: Método que debe contar la frecuencia de las palabras del documento, de acuerdo al `diccionario_universal` y almacenar el resultado en el atributo `vector_caracteristico`.\n",
    "\n",
    "Para que la clase `Documento` sea más \"ligera\", el `diccionario_universal` se define como una variable de clase. Esto quiere decir que es una variable que se comparte entre todas las instancias de la clase. Para definir una variable de clase, basta con definirla fuera de los métodos de la clase, pero dentro de la clase. Por ejemplo:\n",
    "\n",
    "```python\n",
    "class Documento:\n",
    "    diccionario_universal = ['palabra1', 'palabra2', 'palabra3']\n",
    "    \n",
    "    def __init__(self, ruta):\n",
    "        self.ruta = ruta\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionario universal\n",
    "\n",
    "Este diccionario es un conjunto de palabras que debe ser el resultado de analizar todos los documentos y obtener las palabras que aparecen en todos los documentos. De tal manera que este diccionario sirve para poder generar el vector característico de los documentos a procesar.\n",
    "\n",
    "Para fines prácticos, vamos a pensar que el diccionario universal contiene las siguientes palabras, en este orden:\n",
    "\n",
    "```python\n",
    "\n",
    "diccionario_universal = ['factura', 'testamento', 'demanda', 'contrato']\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca17334dc0dd43088510f2daefa570e7",
     "grade": true,
     "grade_id": "cell-bcdc4313a3efcd10",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Documento:\n",
    "    # Variable de clase (diccionario universal)\n",
    "    diccionario_universal = ['factura', 'testamento', 'demanda', 'contrato']\n",
    "    \n",
    "    def __init__(self, ruta):\n",
    "        # Inicialización de atributos\n",
    "        self.ruta = ruta\n",
    "        self.texto = \"\"\n",
    "        self.vector_caracteristico = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        # Representación de la clase en forma de cadena\n",
    "        return f\"Ruta: {self.ruta}\\nVector característico: {self.vector_caracteristico}\"\n",
    "    \n",
    "    def preprocesar(self):\n",
    "        # Lectura y procesamiento del archivo\n",
    "        try:\n",
    "            with open(self.ruta, 'r', encoding='utf-8') as archivo:\n",
    "                # Leer el archivo y convertir el texto a minúsculas\n",
    "                self.texto = archivo.read().lower()\n",
    "        except FileNotFoundError:\n",
    "            print(f\"El archivo {self.ruta} no se encontró.\")\n",
    "    \n",
    "    def data_mining(self):\n",
    "        # Analizar el texto y contar la frecuencia de las palabras del diccionario universal\n",
    "        if not self.texto:\n",
    "            print(\"No hay texto procesado. Ejecuta primero el método preprocesar().\")\n",
    "            return\n",
    "        \n",
    "        # Inicializar el vector característico con ceros\n",
    "        self.vector_caracteristico = [0] * len(self.diccionario_universal)\n",
    "        \n",
    "        # Dividir el texto en palabras usando expresiones regulares para separar por palabras\n",
    "        palabras = re.findall(r'\\b\\w+\\b', self.texto)\n",
    "        \n",
    "        # Contar la aparición de cada palabra exacta del diccionario universal\n",
    "        for i, palabra in enumerate(self.diccionario_universal):\n",
    "            self.vector_caracteristico[i] = palabras.count(palabra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dee18440b878d3ccb580f3155c8cc71",
     "grade": true,
     "grade_id": "cell-b3ec1fde69645766",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "documento = Documento('./Textos/texto1.txt')\n",
    "print(documento.vector_caracteristico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Probar la clase Documento\n",
    "documento = Documento('./Textos/texto1.txt')\n",
    "documento.preprocesar()\n",
    "documento.data_mining()\n",
    "\n",
    "# Imprimir el vector característico\n",
    "print(documento.vector_caracteristico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la clase `Documento`\n",
    "\n",
    "Una vez que se ha definido la clase `Documento`, se debe probar para verificar que funciona correctamente. Para ello, se debe crear una instancia de la clase `Documento` y llamar a los métodos `preprocesar` y `data_mining`. Por ejemplo:\n",
    "\n",
    "```python\n",
    "documento = Documento('./Textos/texto1.txt')\n",
    "print(documento.vector_caracteristico)\n",
    "```\n",
    "\n",
    "Y el resultado debe ser exactamente el siguiente:\n",
    "\n",
    "```\n",
    "[4, 0, 0, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9fa01ddcf0f6d2e97112275c4d756309",
     "grade": true,
     "grade_id": "cell-0b41c598f6dbf864",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m----------------------------------------\n",
      "\u001b[32m1 | Tu resultado es correcto.\n",
      "\u001b[39m----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quizz.eval_numeric('1', documento.vector_caracteristico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 Puntos) Clase `Clasificador`\n",
    "\n",
    "Genera una clase de *python* llamada `Clasificador` que genere una lista de objetos de la clase `Documento`. Esta clase debe tener los siguientes atributos:\n",
    "\n",
    "* `ruta`: Ruta de la carpeta que contiene los documentos a clasificar.\n",
    "* `documentos`: Lista de objetos de la clase `Documento`.\n",
    "\n",
    "Además, debe tener los siguientes métodos:\n",
    "\n",
    "* `__init__(self, ruta)`: Constructor de la clase. Recibe como parámetro la ruta de la carpeta que contiene los documentos a clasificar. Debe inicializar los atributos `ruta` y `documentos`.\n",
    "* `__str__(self)`: Método que devuelve la ruta de la carpeta y el número de documento a clasificar.\n",
    "* `cargar_documentos(self)`: Método que debe leer los documentos de la carpeta que se encuentre en `self.ruta` crear un objeto de tipo `Documento`, por cada documento dentro de la carpeta y almacenarlos en la lista `self.documentos`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55b9424c23c5052c6df6ac70c765ba99",
     "grade": true,
     "grade_id": "cell-84fe234be02c5724",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Clasificador:\n",
    "    def __init__(self, ruta):\n",
    "        # Inicialización de atributos\n",
    "        self.ruta = ruta\n",
    "        self.documentos = []\n",
    "    \n",
    "    def __str__(self):\n",
    "        # Crear una cadena con la información de cada documento\n",
    "        resultado = \"\"\n",
    "        for documento in self.documentos:\n",
    "            resultado += f\"Ruta: {documento.ruta}\\n\"\n",
    "            resultado += f\"Vector caracteristico: {documento.vector_caracteristico}\\n\"\n",
    "        return resultado  # Quita el último salto de línea\n",
    "    \n",
    "    def cargar_documentos(self):\n",
    "        # Leer los archivos de la carpeta y crear objetos Documento\n",
    "        try:\n",
    "            for nombre_archivo in os.listdir(self.ruta):\n",
    "                # Solo procesar archivos de texto\n",
    "                if nombre_archivo.endswith('.txt'):\n",
    "                    ruta_completa = os.path.join(self.ruta, nombre_archivo)\n",
    "                    doc = Documento(ruta_completa)  # Crear un objeto Documento\n",
    "                    doc.preprocesar()  # Procesar el archivo de texto\n",
    "                    doc.data_mining()   # Generar el vector característico\n",
    "                    self.documentos.append(doc)  # Agregar el objeto Documento a la lista\n",
    "        except FileNotFoundError:\n",
    "            print(f\"La carpeta {self.ruta} no se encontró.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ocurrió un error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probando la clase `Clasificador`\n",
    "\n",
    "Una vez que se ha definido la clase `Clasificador`, se debe probar para verificar que funciona correctamente. Para ello, se debe crear una instancia de la clase `Clasificador` y llamar al método `cargar_documentos`. Por ejemplo:\n",
    "\n",
    "```python\n",
    "clasificador = Clasificador('Textos')\n",
    "clasificador.cargar_documentos()\n",
    "print(clasificador)\n",
    "```\n",
    "\n",
    "Y el resultado debe ser exactamente el siguiente:\n",
    "\n",
    "```\n",
    "Ruta: Textos/texto1.txt\n",
    "Vector caracteristico: [4, 0, 0, 0]\n",
    "Ruta: Textos/texto2.txt\n",
    "Vector caracteristico: [2, 0, 0, 0]\n",
    "Ruta: Textos/texto3.txt\n",
    "Vector caracteristico: [10, 0, 0, 0]\n",
    "Ruta: Textos/texto4.txt\n",
    "Vector caracteristico: [0, 0, 0, 7]\n",
    "Ruta: Textos/texto5.txt\n",
    "Vector caracteristico: [0, 0, 0, 5]\n",
    "Ruta: Textos/texto6.txt\n",
    "Vector caracteristico: [0, 0, 0, 4]\n",
    "Ruta: Textos/texto7.txt\n",
    "Vector caracteristico: [0, 0, 4, 2]\n",
    "Ruta: Textos/texto8.txt\n",
    "Vector caracteristico: [0, 0, 2, 1]\n",
    "Ruta: Textos/texto9.txt\n",
    "Vector caracteristico: [0, 7, 0, 0]\n",
    "Ruta: Textos/texto10.txt\n",
    "Vector caracteristico: [0, 11, 0, 0]\n",
    "Ruta: Textos/texto11.txt\n",
    "Vector caracteristico: [0, 4, 0, 0]\n",
    "```\n",
    "\n",
    "Recuerda que la primer entrada de cada vector caracteristico es la frecuencia de la palabra `factura`, la segunda entrada es la frecuencia de la palabra `testamento`, la tercera entrada es la frecuencia de la palabra `demanda` y la cuarta entrada es la frecuencia de la palabra `contrato`.\n",
    "\n",
    "Asi que ya desde este momento puedes comenzar a pensar que clasificación le corresponde a cada documento. Sin embargo, para poder clasificar los documentos de manera automatica, primero se debe entrenar el SOM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50bc2a3bd1598b9360fb5d68acec3f02",
     "grade": true,
     "grade_id": "cell-e5276d28097ee4ac",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ruta: Textos/texto1.txt\n",
      "Vector caracteristico: [4, 0, 0, 0]\n",
      "Ruta: Textos/texto2.txt\n",
      "Vector caracteristico: [2, 0, 0, 0]\n",
      "Ruta: Textos/texto3.txt\n",
      "Vector caracteristico: [10, 0, 0, 0]\n",
      "Ruta: Textos/texto4.txt\n",
      "Vector caracteristico: [0, 0, 0, 7]\n",
      "Ruta: Textos/texto5.txt\n",
      "Vector caracteristico: [0, 0, 0, 5]\n",
      "Ruta: Textos/texto6.txt\n",
      "Vector caracteristico: [0, 0, 0, 4]\n",
      "Ruta: Textos/texto7.txt\n",
      "Vector caracteristico: [0, 0, 4, 2]\n",
      "Ruta: Textos/texto8.txt\n",
      "Vector caracteristico: [0, 0, 2, 1]\n",
      "Ruta: Textos/texto9.txt\n",
      "Vector caracteristico: [0, 7, 0, 0]\n",
      "Ruta: Textos/texto10.txt\n",
      "Vector caracteristico: [0, 11, 0, 0]\n",
      "Ruta: Textos/texto11.txt\n",
      "Vector caracteristico: [0, 4, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clasificador = Clasificador('Textos')\n",
    "clasificador.cargar_documentos()\n",
    "print(clasificador)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b65313d97fe59de5ae30ace999fe614",
     "grade": true,
     "grade_id": "cell-16ce45032803760a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m----------------------------------------\n",
      "\u001b[32m2 | Tu resultado es correcto.\n",
      "\u001b[39m----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quizz.eval_datastruct('2', [str(clasificador)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2 Puntos) Documentos a clasificar\n",
    "\n",
    "Crea una lista de vectores caracteristicos de los documentos que se encuentran en la lista `clasificador.documentos`. Por ejemplo:\n",
    "\n",
    "```python\n",
    "vectores_caracteristicos = [???]\n",
    "```\n",
    "\n",
    "Esta lista debe verse exactamente como la siguiente:\n",
    "\n",
    "```\n",
    "[[4, 0, 0, 0], [2, 0, 0, 0], [10, 0, 0, 0], [0, 0, 0, 7], [0, 0, 0, 5], [0, 0, 0, 4], [0, 0, 4, 2], [0, 0, 2, 1], [0, 7, 0, 0], [0, 11, 0, 0], [0, 4, 0, 0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "feb0166aba989b3344c60952615a113f",
     "grade": true,
     "grade_id": "cell-372cafa447861ee8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0, 0, 0], [2, 0, 0, 0], [10, 0, 0, 0], [0, 0, 0, 7], [0, 0, 0, 5], [0, 0, 0, 4], [0, 0, 4, 2], [0, 0, 2, 1], [0, 7, 0, 0], [0, 11, 0, 0], [0, 4, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "vectores_caracteristicos = [doc.vector_caracteristico for doc in clasificador.documentos]\n",
    "print(vectores_caracteristicos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1caa68e844becb4882c353cf2010a2ac",
     "grade": true,
     "grade_id": "cell-09a2293c14dbb8e5",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m----------------------------------------\n",
      "\u001b[32m3 | Tu resultado es correcto.\n",
      "\u001b[39m----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quizz.eval_numeric('3', vectores_caracteristicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3 Puntos) Entrenando el SOM\n",
    "\n",
    "Crea una instancia de la clase `SOM` con un mapa de $2\\times2$ y con 4 entradas. Para ello, se debe crear una instancia de la clase `SOM`, por ejemplo:\n",
    "\n",
    "```python\n",
    "som = SOM(2, 2, 4, 10)\n",
    "\n",
    "```\n",
    "\n",
    "Ya con el SOM creado, se debe entrenar con los vectores caracteristicos de los documentos. Para ello, se debe llamar al método `train` del objeto `som`. Por ejemplo:\n",
    "\n",
    "```python\n",
    "som.train(vectores_caracteristicos)\n",
    "```\n",
    "\n",
    "Y finalmente, podemos ver la lista que nos devuelve el método `map_vects`. Por ejemplo:\n",
    "\n",
    "```python\n",
    "print(som.map_vects(vectores_caracteristicos))\n",
    "```\n",
    "\n",
    "El resultado der.\n",
    "\n",
    "```\n",
    "[array([1, 1]), array([1, 0]), array([1, 0]), array([0, 0]), array([0, 0]), array([1, 1]), array([0, 0]), array([0, 1]), array([0, 1]), array([1, 1]), array([0, 1])]\n",
    "```\n",
    "\n",
    "Esta es la clasificación que el SOM le ha dado a cada documento. Por ejemplo, el primer documento se encuentra en la posición `[1, 1]` del mapa, el segundo documento se encuentra en la posición `[1, 0]` del mapa, el tercer documento se encuentra en la posición `[1, 0]` del mapa, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.8.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: rich in /home/jovyan/.local/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: namex in /home/jovyan/.local/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/jovyan/.local/lib/python3.11/site-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/jovyan/.local/lib/python3.11/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/jovyan/.local/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/jovyan/.local/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
      "Installing collected packages: tensorflow\n",
      "\u001b[33m  WARNING: The scripts import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed tensorflow-2.17.0\n"
     ]
    }
   ],
   "source": [
    "# EJECUTAR ESTA CELDA\n",
    "!pip install --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac35b9652acc7dad7cde76e60650387",
     "grade": true,
     "grade_id": "cell-00aba5ef02fa0035",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 23:06:27.034773: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-05 23:06:27.039096: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-05 23:06:27.051869: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-05 23:06:27.072070: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-05 23:06:27.078279: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-05 23:06:27.094239: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-05 23:06:30.863265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.11/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    " \n",
    "class SOM(object):\n",
    "    \"\"\"\n",
    "    Clase que representa una red neuronal tipo SOM.\n",
    "    \"\"\"\n",
    " \n",
    "    #Para revisar si la red ya ha sido entrenada\n",
    "    _trained = False\n",
    " \n",
    "    def __init__(self, m, n, dim, n_iterations=100, alpha=None, sigma=None):\n",
    "        \"\"\"\n",
    "        Constructor que toma como parametros los valores descritos en el\n",
    "        algoritmo SOM. Genera un mapa de m renglones por n columnas y se entrenara\n",
    "        con n_iterations\n",
    "        \"\"\"\n",
    " \n",
    "        #Se inicializan variables que seran usadas a lo largo del coidgo\n",
    "        self._m = m\n",
    "        self._n = n\n",
    "        if alpha is None:\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = float(alpha)\n",
    "        if sigma is None:\n",
    "            sigma = max(m, n) / 2.0\n",
    "        else:\n",
    "            sigma = float(sigma)\n",
    "        self._n_iterations = abs(int(n_iterations))\n",
    " \n",
    "        '''SE NECESITA UNA GRAFICA (PLANO), hay una grafica\n",
    "        por default pero la guardamos en _graph'''\n",
    "        self._graph = tf.Graph()\n",
    " \n",
    "        '''SE CREAN LOS ELEMENTOS NECESARIOS EN LA GRAFICA'''\n",
    "        with self._graph.as_default():\n",
    "            '''SE CREAN TODAS LAS NEURONAS CON tf.Variable, son m*n\n",
    "            neuronas con dim pesos, que seran comparados con los pesos\n",
    "            de la entrada y la que tenga la menor distancia sera la\n",
    "            neurona ganadora. Antes de iniciar el entrenamiento, hay\n",
    "            hay que inicializar TODAS las variables'''\n",
    "            \n",
    "            '''Lista de pesos de los vectores de la red neuronal'''\n",
    "            self._weightage_vects = tf.Variable(tf.random.normal(\n",
    "                [m*n, dim]))\n",
    " \n",
    "            '''Lista de 600 entradas, y cada entrada representa una\n",
    "            coordenada en la cual se encuentra cada neurona'''\n",
    "            self._location_vects = tf.constant(np.array(\n",
    "                list(self._neuron_locations(m, n))))\n",
    " \n",
    "            '''self._vect_input es un placeholder de tamano dim, ya que\n",
    "            es el objeto que sera alimentado con el vector de entrada y\n",
    "            a su vez este sera comparado con los pesos de cada neurona.\n",
    "            Esto es asi por el framework que da tensorflow'''\n",
    "            self._vect_input = tf.placeholder(\"float\", [dim])\n",
    "            \n",
    "            '''Lo mismo sucede con esta variable, la diferencia es que en\n",
    "            este punto aun no se sabe cuantas iteraciones (epocas) seran\n",
    "            necesarias, asi que se deja en cero.'''\n",
    "            self._iter_input = tf.placeholder(\"float\")\n",
    " \n",
    "            '''Devuelve el indice con el menor valor, es decir la neurona mas cercana.'''\n",
    "            bmu_index = tf.argmin(tf.sqrt(tf.reduce_sum(\n",
    "                tf.pow(tf.subtract(self._weightage_vects, tf.stack(\n",
    "                    [self._vect_input for i in range(m*n)])), 2), 1)),\n",
    "                                  0)\n",
    " \n",
    "            '''Variable que guarda el indice y un espacio para el sus\n",
    "            coordenada'''\n",
    "            slice_input = tf.pad(tf.reshape(bmu_index, [1]),\n",
    "                                 np.array([[0, 1]]))\n",
    "            bmu_loc = tf.reshape(tf.slice(self._location_vects, slice_input,\n",
    "                                          tf.constant(np.array([1, 2]))),\n",
    "                                 [2])\n",
    " \n",
    "            '''Valores necesario para actualizar los pesos de las neuronas\n",
    "            de acuerdo a la iteracion (epoca)'''\n",
    "            learning_rate_op = tf.subtract(1.0, tf.div(self._iter_input,\n",
    "                                                  self._n_iterations))\n",
    "            _alpha_op = tf.multiply(alpha, learning_rate_op)\n",
    "            _sigma_op = tf.multiply(sigma, learning_rate_op)\n",
    " \n",
    "            '''Calcula las distancias al cuadrado por cada neurona con respecto\n",
    "            a la neurona GANADORA (BMU). De tal manera que estos valores\n",
    "            puedan ser empleados para actualizar los pesos de los vecinos'''\n",
    "            bmu_distance_squares = tf.reduce_sum(tf.pow(tf.subtract(\n",
    "                self._location_vects, tf.stack(\n",
    "                    [bmu_loc for i in range(m*n)])), 2), 1)\n",
    "            neighbourhood_func = tf.exp(tf.negative(tf.div(tf.cast(\n",
    "                bmu_distance_squares, \"float32\"), tf.pow(_sigma_op, 2))))\n",
    "            learning_rate_op = tf.multiply(_alpha_op, neighbourhood_func)\n",
    " \n",
    "            '''Tasa de aprendizaje para actualizar los pesos de las neuronas'''\n",
    "            learning_rate_multiplier = tf.stack([tf.tile(tf.slice(\n",
    "                learning_rate_op, np.array([i]), np.array([1])), [dim])\n",
    "                                               for i in range(m*n)])\n",
    "            weightage_delta = tf.multiply(\n",
    "                learning_rate_multiplier,\n",
    "                tf.subtract(tf.stack([self._vect_input for i in range(m*n)]),\n",
    "                       self._weightage_vects)) \n",
    "            \n",
    "            '''Actualiza todos los pesos de las neuronas de acuerdo a los\n",
    "            parametros calculados previamente'''                                        \n",
    "            new_weightages_op = tf.add(self._weightage_vects,\n",
    "                                       weightage_delta)\n",
    "            \n",
    "            '''Se guarda la ultima operacion realizada en la SOM, ya que\n",
    "            esta operacion sera la que se ejecute y a su vez ejecuta todas\n",
    "            las operaciones previar al llamar a sess.run()'''\n",
    "            self._training_op = tf.assign(self._weightage_vects,\n",
    "                                          new_weightages_op)                                       \n",
    " \n",
    "            '''En tensorflow todo debe ocurrir dentro de una sesion, es por\n",
    "            este motivo que se guarda la sesion'''\n",
    "            self._sess = tf.Session()\n",
    " \n",
    "            '''Forma en la tensorflow inicializa sus variables antes de ser\n",
    "            utilizadas'''\n",
    "            init_op = tf.initialize_all_variables()\n",
    "            self._sess.run(init_op)\n",
    "            \n",
    "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
    "            valores de las neuronas. Es de tamano m, por que para cada renglon\n",
    "            se tienen n neuronas y sus respectivos valores. '''\n",
    "            centroid_grid = [[] for i in range(self._m)]\n",
    "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "            self._locations = list(self._sess.run(self._location_vects))\n",
    "    \n",
    "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
    "            en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
    "            ser facilmente graficado por matplotlib. Es el mapa incial (SIN ENTRENAR)'''\n",
    "            for i, loc in enumerate(self._locations):\n",
    "                centroid_grid[loc[0]].append(self._weightages[i])\n",
    "            self._mapa_inicial = centroid_grid\n",
    " \n",
    "    def _neuron_locations(self, m, n):\n",
    "        '''Yield regresa un generador flojo, y hasta que es necesario\n",
    "        se evalua. Esto se hace para que no haya informacion no necesaria\n",
    "        en memoria. En el constructor el resultado de esta funcion se\n",
    "        mete en una lista para que sea accesible de inmediato'''\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                yield np.array([i, j])\n",
    " \n",
    "    def train(self, input_vects, debbug=False):\n",
    "        if not debbug:\n",
    "            #centroid_grid = [[] for i in range(self._m)]\n",
    "            '''Para cada iteracion (epoca) se realiza el entrenamiento'''\n",
    "            for iter_no in range(self._n_iterations):\n",
    "                actual = self._sess.run(tf.norm(self._weightage_vects))\n",
    "                #Se entrena con cada vector uno por uno\n",
    "                for input_vect in input_vects:\n",
    "                    self._sess.run(self._training_op,\n",
    "                                  feed_dict={self._vect_input: input_vect,\n",
    "                                              self._iter_input: iter_no})\n",
    "                siguiente = self._sess.run(tf.norm(self._weightage_vects))\n",
    "                '''Si la norma del mapa actual no varia mucho con respecto\n",
    "                al siguiente, se rompe el ciclo de las epocas'''\n",
    "                if abs(siguiente - actual) <= 0.000001:\n",
    "                    break\n",
    "            '''centroid_grid es un mapa de bits en el cual se guardan los\n",
    "                valores de las neuronas. Es de tamano m, por que para cada renglon\n",
    "                se tienen n neuronas y sus respectivos valores. '''\n",
    "            centroid_grid = [[] for i in range(self._m)]\n",
    "            self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "            self._locations = list(self._sess.run(self._location_vects))\n",
    "            \n",
    "            '''Con este for, se accede a cada neurona por posicion y se guarda\n",
    "                en centroid_grid sus pesos. El resultado es un mapa de bits que puede\n",
    "                ser facilmente graficado por matplotlib. En este punto la red ya esta entrenada.'''\n",
    "            for i, loc in enumerate(self._locations):\n",
    "                centroid_grid[loc[0]].append(self._weightages[i])\n",
    "            self._centroid_grid = centroid_grid\n",
    "    \n",
    "            '''En este punto la red ya esta entrenada.'''\n",
    "            self._trained = True\n",
    "            '''Esta seccion muestra como se entrena el SOM y es basicamente el mismo\n",
    "            codigo de la seccion del if y al final solo se agrega la grafica del mapa.'''\n",
    "        else:\n",
    "            centroid_grid = [[] for i in range(self._m)]\n",
    "\n",
    "            for iter_no in range(self._n_iterations):\n",
    "                actual = self._sess.run(tf.norm(self._weightage_vects))\n",
    "                #Se entrena con cada vector uno por uno\n",
    "                for input_vect in input_vects:\n",
    "                    self._sess.run(self._training_op,\n",
    "                                  feed_dict={self._vect_input: input_vect,\n",
    "                                              self._iter_input: iter_no})\n",
    "                siguiente = self._sess.run(tf.norm(self._weightage_vects))\n",
    "\n",
    "                if abs(siguiente - actual) <= 0.000001:\n",
    "                    break\n",
    "                if iter_no % 10 == 0:    \n",
    "                    centroid_grid = [[] for i in range(self._m)]\n",
    "                    self._weightages = list(self._sess.run(self._weightage_vects))\n",
    "                    self._locations = list(self._sess.run(self._location_vects))\n",
    "                    \n",
    "                    for i, loc in enumerate(self._locations):\n",
    "                        centroid_grid[loc[0]].append(self._weightages[i])\n",
    "                    self._centroid_grid = centroid_grid\n",
    "\n",
    "                    red_entrenada = som.get_centroids()\n",
    "                    # SECCION PARA GRAFICAR\n",
    "                    bmu = self.map_vect(input_vect)\n",
    "                    plt.text(bmu[1], bmu[0], \"bmu\", ha='center', va='center',\n",
    "                        bbox=dict(facecolor='white', alpha=0.5, lw=0))\n",
    "                    plt.imshow(red_entrenada)\n",
    "                    plt.show()\n",
    "                    input(\"Continuar?\")\n",
    "    \n",
    "            '''En este punto la red ya esta entrenada.'''\n",
    "            self._trained = True\n",
    " \n",
    "    def get_centroids(self):\n",
    "        # Solo devuelve los centroides para que puendan ser graficados\n",
    "        #if not self._trained:\n",
    "            #raise ValueError(\"La red aun no ha sido entrenada\")\n",
    "        return self._centroid_grid\n",
    " \n",
    "    def map_vects(self, input_vects):\n",
    "        '''to_return es la lista que contiene las coordenadas (x,y) de la\n",
    "        neurona que mas se parece a cada una de las entradas de input_vects\n",
    "        en el mismo orden'''\n",
    " \n",
    "        if not self._trained:\n",
    "            raise ValueError(\"SOM not trained yet\")\n",
    " \n",
    "        to_return = []\n",
    "        for vect in input_vects:\n",
    "            min_index = min([i for i in range(len(self._weightages))],\n",
    "                            key=lambda x: np.linalg.norm(vect-\n",
    "                                                         self._weightages[x]))\n",
    "            to_return.append(self._locations[min_index])\n",
    " \n",
    "        return to_return\n",
    "    \n",
    "    def map_vect(self, vect):\n",
    "        '''\n",
    "        Mapea un solo vector y devuelve la clasificacion vista como\n",
    "        un indice relacionado a la coordenada (x,y) de la neurona\n",
    "        '''\n",
    "\n",
    "        min_index = min([i for i in range(len(self._weightages))],\n",
    "                        key=lambda x: np.linalg.norm(\n",
    "                            vect - self._weightages[x]))\n",
    "        pos2D = self._locations[min_index]\n",
    "        # polinomio de direccionamiento de la neurona\n",
    "        #return pos2D[0]*self._m + pos2D[1], pos2D\n",
    "        return (pos2D[1], pos2D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a438e67cac558540867d6ba5e1c6478a",
     "grade": true,
     "grade_id": "cell-66ba2970db7903ad",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "WARNING:tensorflow:From /home/jovyan/.local/lib/python3.11/site-packages/tensorflow/python/util/tf_should_use.py:288: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-05 23:06:41.459494: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0, 1]), array([0, 1]), array([0, 1]), array([1, 1]), array([1, 1]), array([1, 1]), array([1, 1]), array([1, 1]), array([1, 0]), array([1, 0]), array([0, 0])]\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Crear una instancia de SOM (2x2 y con vectores de 4 dimensiones)\n",
    "som = SOM(2, 2, 4, 100)\n",
    "\n",
    "# Paso 2: Entrenar el SOM con los vectores característicos de los documentos\n",
    "som.train(vectores_caracteristicos)  # 'lista_vectores' es la lista que generamos antes\n",
    "\n",
    "# Paso 3: Ver la clasificación de los documentos\n",
    "clasificacion = som.map_vects(vectores_caracteristicos)\n",
    "print(clasificacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dada la naturaleza aleatoria inicial del mapa, es posible que tu clasificación sea diferente a la que se muestra arriba. Sin embargo, si se ha hecho todo correctamente, la clasificación debe ser muy similar a la que se muestra arriba.\n",
    "\n",
    "Finalmente mediante el polinomio de direccionamiento, se puede \"aplanar\" el mapa y obtener una lista de clasificaciones. Por ejemplo:\n",
    "\n",
    "```python\n",
    "clasificaciones = [0,0,0,0]\n",
    "for clas in mapeados:\n",
    "    # polinomio de direccionamiento para aplanar el mapa\n",
    "    clasificaciones[clas[0]*som._m + clas[1]] += 1\n",
    "print(clasificaciones)\n",
    "```\n",
    "\n",
    "Si ordenas la lista de clasificaciones de mayor a menor y todo funciona de manera correcta el resultado que verás será el siguiente:\n",
    "\n",
    "```python\n",
    "[3, 3, 3, 2]    \n",
    "```\n",
    "\n",
    "Lo que significa que tenemos 3 documentos clasificados como `contrato`, 3 documentos clasificados como `factura`, 3 documentos clasificados como `testamento` y 2 documentos clasificados como `demanda`. Si no me crees, puedes revisar manualmente cada documento y verificar que la clasificación es correcta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aaaa53fad360324f901fff34d1e0dab6",
     "grade": true,
     "grade_id": "cell-a81f980e596c5e60",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 3, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "clasificaciones = [0,0,0,0]\n",
    "for clas in clasificacion:\n",
    "    # polinomio de direccionamiento para aplanar el mapa\n",
    "    clasificaciones[clas[0]*som._m + clas[1]] += 1\n",
    "clasificaciones = sorted(clasificaciones, reverse = True)\n",
    "print(clasificaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aac027b1031512109a1eb7b4024cea8c",
     "grade": true,
     "grade_id": "cell-9e27ba06390ba1a5",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[39m----------------------------------------\n",
      "\u001b[32m4 | Tu resultado es correcto.\n",
      "\u001b[39m----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "quizz.eval_numeric('4', clasificaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observaciones\n",
    "\n",
    "Para facilitar el desarrollo de este ejercicio se proporcionan elementos que deberían obtenerse de los datos a procesar, como por ejemplo el diccionario de palabras, la lista de documentos, etc. Sin embargo, si se desea, este ejercicio se puede usar como base para un proyecto más ambicioso, en el que se procesen documentos reales.\n",
    "\n",
    "Finalmente el polinomio de direccionamiento es una manera de \"aplanar\" el mapa y obtener una lista de clasificaciones.\n",
    "\n",
    "Se deja como ejercicio para el lector, incrementar la dificultad de este ejercicio, procesando documentos o usando tamaños de mapa más grandes para obtener mejores resultados, además de diccionarios más grandes para obtener clasificaciones más precisas.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://github.com/jugernaut/ManejoDatos/blob/desarrollo/Imagenes/AlgoritmosBusqueda/traemethanos.gif?raw=1\" width=\"600\">   \n",
    "</center>\n",
    "\n",
    "\"¡Traiganme a Thanos!\"\n",
    "-Thor, infinity war."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
